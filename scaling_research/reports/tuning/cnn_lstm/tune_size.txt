--- 8653.796469688416 seconds --
feature_no = 300
cnn_seq = 15
1 cnn, 1 lstm
Best: -181333955.110093 using {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-181333955.110093 (82498658.663540) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-111546396106938.484375 (157748376119856.468750) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-1731899424656677333905702912.000000 (2449275655015633162881466368.000000) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 750}

- exploding gradients

added gradient clipping 

model = Sequential()
model.add(TimeDistributed(Conv1D(filters=32, kernel_size=1, activation='relu'), input_shape=(None, self.cnn_steps_per_seq, 1)))
model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
model.add(TimeDistributed(Flatten()))
model.add(LSTM(lstm_units, activation='relu'))
model.add(Dense(1))
ada_dlt = optimizers.Adadelta(clipnorm=1.)
model.compile(optimizer=ada_dlt, loss='mse',  metrics=['mse', 'mae', 'mape'])
return model

--- 14975.336343288422 seconds ---
feature_no = 300
cnn_seq = 15
Best: -246081628.198248 using {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-255283366.089361 (168742590.385908) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-246081628.198248 (180881276.231587) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-393858807.106501 (140133545.746642) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 750}
