--- 8653.796469688416 seconds --
feature_no = 300
cnn_seq = 15
1 cnn, 1 lstm
Best: -181333955.110093 using {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-181333955.110093 (82498658.663540) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-111546396106938.484375 (157748376119856.468750) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-1731899424656677333905702912.000000 (2449275655015633162881466368.000000) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 750}

-- 18193.088843107224 seconds --- ran on my laptop ; same code; weird performance? - did not reach similar with/without gradient clipping
feature_no = 256
cnn_seq = 16
1cnn, 1lstm
Best: -55043137.171994 using {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-55043137.171994 (25797225.678789) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-303639207.371956 (316183260.136144) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-93652777119847567851520.000000 (132445027556792828166144.000000) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 750}

- exploding gradients

added gradient clipping 

model = Sequential()
model.add(TimeDistributed(Conv1D(filters=32, kernel_size=1, activation='relu'), input_shape=(None, self.cnn_steps_per_seq, 1)))
model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
model.add(TimeDistributed(Flatten()))
model.add(LSTM(lstm_units, activation='relu'))
model.add(Dense(1))
ada_dlt = optimizers.Adadelta(clipnorm=1.)
model.compile(optimizer=ada_dlt, loss='mse',  metrics=['mse', 'mae', 'mape'])
return model

--- 14975.336343288422 seconds ---
feature_no = 300
cnn_seq = 15
Best: -246081628.198248 using {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-255283366.089361 (168742590.385908) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-246081628.198248 (180881276.231587) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-393858807.106501 (140133545.746642) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 750}

--- 12798.11640739441 seconds ---
feaure_no = 144
cnn_seq = 12
Best: -61837065.504840 using {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-137240625.939130 (67641049.459338) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-61837065.504840 (13591701.989263) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-93691570.655383 (22620531.907937) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 750}

--- truncated
feature_no= 256
cnn_seq = 16
-98380243.241503 (48442079.224377) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-161887410.139187 (94762638.267732) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-333100702.983231 (349340167.012960) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 750}

--- 8782.48883318901 seconds ---
feature_no = 64
cnn_seq = 8
Best: -370621888.125939 using {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-430070436.071842 (15692643.942598) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 150}
-370621888.125939 (110194022.487403) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 500}
-406012554.663382 (136238961.795293) with: {'batch_size': 8, 'epochs': 100, 'lstm_units': 750}
