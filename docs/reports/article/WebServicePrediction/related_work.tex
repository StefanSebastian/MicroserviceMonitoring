Calheiros et al.\cite{arima_prediction} apply the ARIMA model
  to cloud workload prediction. The model was evaluated using a trace of English Wikipedia resource requests
  spanning a duration of four weeks. The first three are used for training and the fourth for prediction 
  using a time window of 1 hour. The MAPE varies from 9\% to 22\% depending on the confidence interval chosen, 
  from 80 to 95 which is meant to limit the occurences of underestimations.

  Other classic timeseries models have also been applied for this task, like Brown Exponential 
  Smoothing by Mi et al.\cite{brown_prediction} obtaining a Mean Relative Error of 0.064 on 
  the France World Cup 1998 web server trace. Another classic model is Weighted Moving Average,
  applied by Aslanpour et al.\cite{wma_prediction} 
  in which recent observations are given more weight based on the Fibonacci rule, and was tested 
  on a NASA server 24h trace achieving a 5\% improvement in response time on a cloud scaling 
  simulator.

  Kumar and Singh\cite{ann_prediction} applied artificial neural networks 
  for workload prediction on a seven month log of traffic from a Saskatchewan University 
  web server and a two month one from the NASA Kennedy Space Center web server. They use 
  a classic ANN architecture : one input layer(size 10), one hidden and one output, and the model 
  is trained through the SaDE technique, which means learning its weight through evolutionary algorithms.
  The results of this model were compared to an ANN trained through backpropagation: 0.013 and 0.001 for D1 and
  D2 vs 0.265 and 0.119, using the RMSE metric over normalized data.

  CloudInsight\cite{CloudInsight} is one of the most complex models for workload prediction. It uses a technique called "council 
  of experts", meaning an ensemble of different models, in this case: classic timeseries (autoregressive, moving average, exponential smoothing),
  linear regression, and machine learning (SVM). Each model has a different prediction weight which is also learned real-time through 
  a SVM based on their accuracy on the dataset. The evaluation was done on a subset of the wikipedia trace\cite{wikidata}, on google 
  cloud data and on some generated workloads. An indicator of performance is normalized RMSE, meaning how much better it performs than 
  other models. On average it was 13\% to 27\% better than baselines (ARIMA, FFT, SVM, RSLR).
  
 
A review of how deep learning methods can be applied to time series problems was created by Gamboa\cite{dl_ts}.
The paper distinguishes between three types of problems: classification, forecasting and anomaly detection, presents
methods for modelling them and guidance for selecting appropiate models. It also shows an improvement
in performance over the previously existing techniques. Brownlee\cite{dlts_book} published a 
comprehensive guide on applying MLPs, CNNs and LSTMs on various real datasets and discussed their 
advantages over classic methods, which were used as baselines for the experiments. 

Lin et al\cite{cnn_lstm} proposed a hybrid CNN LSTM architecture for learning trend in time series.
It relies on on CNN to extract important features from raw timeseries data and LSTM to 
find long range dependencies in historical data. The model was shown to outperform both CNN and LSTM 
with around 30\% lower RMSE on 3 real world datasets.