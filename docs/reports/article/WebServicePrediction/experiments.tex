
In this section we will described the experiments that we conducted, their results and a comparative analysis of these results.

We have chosen for 
%model tuning and validation  
the {\em Japanese wikipedia trace ????} because besides that fact that includes significant patterns, it also has some interesting irregularities, like a huge spike which is not repeated. 

A realistic workload has been used -- a wikipedia trace for 12 days
in september 2007. From this data set, a subset of requests was extracted (all requests for Japanese
wikipedia). The subset data was also used by Kim et al. \cite{KWQH18} and we intend to compare to obtained results. 



\subsection{Settings}
The selected time window for tuning is 10 min. (todo test on other
windows - tune on other windows?)

The dataset is split into a training and testing with a ratio of 0.9. 

The validation
method is k-fold Cross-Validation [10] with k = 3, which means splitting the training
dataset into k equal parts, perform training on k - 1 and evaluation on the part left out.

This process is repeated k times. 

In order to avoid  the model to be dynamically influenced, we do not modify the testing data during the model tuning.

\subsection{Naive baseline}

\subsection{ARIMA}


	\begin{figure}
	\centering{
		\includegraphics[scale=0.45]{}
	}
	\caption{The results of ARIMA .}
	\label{fig:arima1}
\end{figure}

	\begin{figure}
	\centering{
		\includegraphics[scale=0.45]{}
	}
	\caption{Autocorrelation plot.}
	\label{fig:arima2}
\end{figure}
Settings

Results

The series stationarity was checked using the augmented Dickey-
Fuller test [2] which found the p-value to be 1.0902496274664773e-08. This is lower than
0.05, the commonly used threshold, meaning we can set d to 0.
The partial auto-correlation plot was analyzed to set the auto-regression parameter (p).
From Figure 2 we can see that the significance region is confidently passed at 1, with a
steep decline afterwards. The moving average parameter (q) is approximated from the
auto-correlation plot. It suggests a value of around 20 would be a good start. After fitting
ARIMA(1,0,20) the final 2 layers had P-value of 0.547 and 0.758 which meant that they
were not significant. After trying some values for q: 5,10,15,18 the best results were
obtained on ARIMA(1,0,15) with MSE: 15289199.210, MAE: 3118.982, MAPE: 6.782.


\subsection{MLP}
Settings

Results

After some manual experiments started with a MLP with 2 hidden layers (150, 100 neurons)
and sliding window size of 24 (input size).
To find an optimal combination of batch size and epoch no a 2d grid search was
performed 3. Batch size should ideally be a power of 2 for extra performance on GPU
architectures, as some experiments were ran on Google Colab. Lower batch size is more
accurate but training is slower [6]. As expected the best MSE is obtained for the lowest
batch size(4) however it does not drop significantly at 8, regardless of epochs no. The
selection of epoch no is again a trade-off between speed and accuracy. We see a smaller no
of epochs(50) performs poorly, while the difference between 100 and 250 is not that great,
meaning that we can get a good approximation of a model using a batch size of 100.
Some experiments were done with adding Dropout layers on different values (0.2,
0.1, 0.05) however it did not improve performance. These are generally used to prevent
overfitting, when the network is too big, the data is scarce or training is done for too
long [11], which was not the case for this experiment.
Various optimizer and activation functions were tested. The Adadelta optimizer and
the relu activation were selected. A comprehensive grid search was performed for sliding
window size and number and content of hidden layers, of around 90 combinations. Some
of the best performing are presented in table 2.

\subsection{CNN}

Settings

Results
\subsection{CNN-LSTM Hybrid}

The starting values for some parameters
were influenced by the research done by Lin et al. [9]: 32 cnn filters, 1 lstm layer with a couple hundred units.

The results are presented in Figure \ref{fig:CNN-LSTM}

\subsection{Analysis}

\textcolor{red}{final analysis of the results}

 \begin{table}[htbp]
 \caption{ Final results}
 \begin{center}
 \begin{tabular}{|c|c|c|c|c|c|}
 \hline
DataSet & Naive & ARIMA & MLP & CNN & CNN-LSTM \\
\hline
Jp10& 20233320 & 15289199 & 8591086 & 12766376 & 7252806\\
\hline
Jp15 & 87883950 & 56662039 & 31042348 & 36278866 & todo\\
\hline
De10 & 16789198 & 10318406 & 5180340 & todo & todo\\
\hline
De15 & 77481580 & 43398336 & 17276322 & todo & todo\\
\hline
 \end{tabular}
 \label{tabel_LevelsOfLearningFundamental}
 \end{center}
 \end{table}