

\cite{KWQH18}

The  methodology for tuning the models is based on the following main stages:

\begin{itemize}
    \item choose a realistic data set...
    \item prepare the data such that ...
   \item apply baseline models
   \item apply CNN models
   \item compare and analyse de results
\end{itemize}

\subsection*{Data preparation}

 In order to turn a web request log file into a supervised dataset the following steps were,
    taken: 
\begin{itemize}
\item create buckets which contain the number of requests in a
time interval, 
\item iterate over the buckets using the sliding window technique [1]. 
\end{itemize}

Basically
we generate training instances with input (t, t-1, ... t-n) and output (t+2). The predicted
value is t+2 instead of t+1 because a scaler using this model would need to have a buffer
window during which to deploy the services.

\subsection{Performance Metrics}
The error metrics selected for this experiment are: 
Mean squared error, $MSE=1/n \sum_{i=1}^{n}(Y_i - \hat{Y_i})^2$, 
Mean absolute error, $MAE=1/n \sum_{i=1}^{n}|Y_i - \hat{Y_i}|$, 
and Mean absolute percentage error, 
$MAPE=1/n \sum_{i=1}^{n}(Y_i - \hat{Y_i})/Y_i$,
 as described in \cite{error_metrics}. MSE was used as the 
loss function for training because it tends to penalize big deviations in prediction, which is desirable for 
our problem as we want to accurately predict traffic spikes. MAE is similar, but conceptually simpler, 
given that each prediction error contributes in proportion to its absolute value. 

MAPE is independent of the problem scale and 
can be interpreted intuitively, therefore can be used to give a general impression of how well a model
performs across diferrent datasets. According to \cite{mape_values} a highly accurate forecast would 
have MAPE lower 10\%, and a good forecast between 10 and 20\%.

\subsection{Baseline models}
 Two baseline models were applied: the naive approach
(predict traffic in window t+2 to be traffic in t) and a classic timeseries model (ARIMA).

ARIMA [4], which stands for autoregressive integrated moving average, is a classic approach
to modelling timeseries. In order to apply this model we need to nd appropiate
values for its parameters: p, q, d.
The value of d means the number of times the series needs to be differentiated in order
to make it stationary. 

\textcolor{blue}{The series stationarity was checked using the augmented Dickey-
Fuller test [2] which found the p-value to be 1.0902496274664773e} -??? experiments????

These baseline models have been chosen since they are considered....

\subsection{Deep Learning Models}

We have chosen in this investigation to apply the following deep learning architectures:MLP, CNN, CNN-LSTM hybrid  \textcolor{red}{details}.

\textcolor{blue}{These architectures were chosen for their characteristics and ... previous work}

\subsubsection{MLP}

\subsubsection{CNN}

\subsubsection{CNN-LSTM Hybrid}
This model was applied on a range of timeseries tasks by Lin et al. [9]. It relies on
LSTM to find long range dependencies and historical trends and on CNN to extract important features from raw timeseries data. 

The starting values for some parameters
were influenced by the research done by Lin et al. [9]: 32 cnn filters, 1 lstm layer with a couple hundred units.

