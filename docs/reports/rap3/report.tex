\documentclass[12pt]{article}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{caption}

\linespread{1.25} % the equivalent of 1.5 line spacing from msword
\usepackage[a4paper, left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry} % margins


\title{Web service workload prediction using deep learning - Report 3}
\date{\today}
\author{Stefan Sebastian}

\begin{document}
  \pagenumbering{gobble}
  \maketitle
  
  \newpage
  \tableofcontents
  \newpage 
  \pagenumbering{arabic}

  \section{State of the art}
  TODO 

  \section{Approach}
  The main goal is to find a performant model for web application 
  workload prediction, which can be later used by a proactive 
  microservice scaler. The methods used are different architectures
  of deep learning models: MLP, CNN, CNN-LSTM hybrid. The main 
  contribution of this research is the application of deep learning 
  to this specific problem and the comparison with a classic timeseries
  approach (ARIMA).

  The problem design has been influenced by the goal of integrating
  this model into a proactive microservice scaler. First of all,
  the choice of the workload measure is number of requests. The idea
  is that the scaling prediction should not influence the predicted 
  value, as would be the case with CPU or memory usage. Also this is 
  in line with research done by Jindal et al\cite{msc} who propose 
  a metric for measuring microservice performance based on number of 
  satisfied requests. Another consideration is the prediction interval.
  Taking into account the experience of Netflix\cite{scryer}, who run 
  a microservice architecture in production, the time window should be in 
  the order of minutes, so you can predict spikes and have time to deploy
  new service instances.

  A realistic workload has been used for this experiment, a wikipedia 
  trace for 12 days in september 2007. From this a subset of requests was 
  extracted (all requests for Japanese wikipedia). The subset was selected 
  in order to compare results with Kim et al.\cite{CloudInsight} which used 
  the same dataset. In order to turn a web request log file into a supervised 
  dataset the following steps were taken: create buckets which contain the 
  number of requests in a time interval, iterate over the buckets using 
  the sliding window technique\cite{sliding_window}. Basically we generate 
  training instances with input (t, t-1, ... t-n) and output (t+2). The 
  predicted value is t+2 instead of t+1 because a scaler using this model 
  would need to have a buffer window during which to deploy the services.

  After preparing the dataset two baseline models were prepared: the naive 
  approach (predict traffic in window t+2 to be traffic in t) and a classic 
  timeseries model (ARIMA). The next step was to experiment with different 
  deep learning architectures and measure the results.


  \section{Evaluation of the approach}
  \subsection{Validation and tuning}
  Model tuning and validation was done on the Japanese wikipedia trace because 
  although presenting some patterns it also has some interesting irregularities, 
  like a huge spike which is not repeated. The selected time window for tuning 
  is 10 min. (todo test on other windows - tune on other windows?)

  The dataset is split into a training and testing with a ratio of 0.9.
  The validation method is k-fold Cross-Validation\cite{kfold} with k = 3, 
  which means splitting the training dataset into k equal parts, perform training
  on k - 1 and evaluation on the part left out. This process is repeated k times.
  The main idea is to not touch the testing data while tuning the model, so the 
  model will not be influenced by it.

  \subsection{Performance metrics}
  TODO detail mse, mape, mae


  \subsection{Baseline}
  \subsubsection{Naive baseline}
  A naive baseline is set, to get an idea if the models are useful at all. The 
  naive predictor simply states that traffic in window t+2 will be the same as 
  in the last measured window. The prediction is ploted in figure \ref{fig:baseline_pred}
  and achieves MSE: 20233320.341, MAE: 3577.844, MAPE: 7.106.

  \begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
      \includegraphics[width=\linewidth]{resources/baseline/naive_predictions_10.png}
      \caption{Naive prediction}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
      \includegraphics[width=\linewidth]{resources/baseline/arima_10.png}
      \caption{Arima prediction}
    \end{subfigure}
    \caption{Baseline predictions}
    \label{fig:baseline_pred}
  \end{figure}

  \subsubsection{ARIMA}
  ARIMA\cite{arima}, which stands for autoregressive integrated moving average, is a classic approach 
  to modelling timeseries. In order to apply this model we need to find appropiate values for its 
  parameters: p, q, d. 
  
  The value of d means the number of times the series needs to be differentiated
  in order to make it stationary. The series stationarity was checked using the augmented Dickey-Fuller 
  test\cite{Dickey-Fuller} which found the p-value to be 1.0902496274664773e-08. This is lower than 0.05, 
  the commonly used threshold, meaning we can set d to 0. 
  
  The partial autocorrelation plot was analyzed
  to set the autoregression parameter (p). From figure \ref{fig:arima_params} we can see that the significance
  region is confidently passed at 1, with a steep decline afterwards. The moving average parameter (q) is 
  approximated from the autocorrelation plot. It suggests a value of around 20 would be a good start. 
  After fitting ARIMA(1,0,20) the final 2 layers had P-value of 0.547 and 0.758 which meant that they were 
  not significant. After trying some values for q: 5,10,15,18 the best results were obtained on ARIMA(1,0,15)
  with 14263566.564, MAE: 3056.765, MAPE: 6.349.

  \begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\linewidth}
      \includegraphics[width=\linewidth]{resources/baseline/arima_10_acf.png}
      \caption{Autocorrelation plot}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\linewidth}
      \includegraphics[width=\linewidth]{resources/baseline/arima_10_pacf.png}
      \caption{Partial autocorrelation plot}
    \end{subfigure}
    \caption{ARIMA params}
    \label{fig:arima_params}
  \end{figure}

  \subsection{Deep learning models}

  \subsubsection{MLP}
  After some manual experiments started with a MLP with 2 hidden layers (150, 100 neurons) and sliding window
  size of 24 (input size). 

  To find an optimal combination of batch size and epoch no a 2d grid search was performed \ref{fig:epoch_batch}.
  Batch size should ideally be a power of 2 for extra perfromance on GPU architectures, as some 
  experiments were ran on Google Colab. Lower batch size is more accurate but training is slower\cite{batch_size}.
  As expected the best MSE is obtained for the lowest batch size(4) however it does not drop significantly
  at 8, regardless of epochs no. The selection of epoch no is again a tradeoff between speed and accuracy. 
  We see a smaller no of epochs(50) performs poorly, while the difference between 100 and 250 is not that great,
  meaning that we can get a good approximation of a model using a batch size of 100.

  \begin{figure}
    \includegraphics[width=\linewidth]{resources/mlp/epochs_batch.png}
    \caption{2d grid search for epoch no and batch size}
    \label{fig:epoch_batch}
  \end{figure}

  Some experiments were done with adding Dropout layers on different values (0.2, 0.1, 0.05) however it did 
  not improve performance. These are generally used to prevent overfitting, when the network is too big, the 
  data is scarce or training is done for too long\cite{dropout}, which was not the case for this experiment.

  Various optimizer and activation functions were tested. The Adadelta optimizer and the 
  relu activation were selected. A comprehensive grid search was performed for sliding window size and number 
  and content of hidden layers, of around 90 combinations. Some of the best performing are presented 
  in table \ref{tab:layers}.

  \begin{table}
    \caption{Selecting optimizer and activation. Scores are averaged MSE.}
    \begin{minipage}{.5\linewidth}
      \caption*{Optimizer}
      \centering
        \begin{tabular}{ll}
            RMSprop & 24734800\\
            Adadelta & 19291753\\
            Adagrad & 190604349\\
            Adam & 25828119\\
            Adamax & 29578706\\
            Nadam & 20400557
        \end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
        \caption*{Activation function}
        \begin{tabular}{ll}
            softmax & 4875804739\\
            softplus & 20314197\\
            softsign & 4034049993\\
            relu & 19571788\\
            tanh & 4175933232\\
            sigmoid & 3055609656\\
            linear & 20661311
        \end{tabular}
    \end{minipage}
  \end{table}

  \begin{table}
    \begin{center}
      \caption{MLP layers and size tuning}
      \label{tab:layers}
      \begin{tabular}{c|c|c}
        \textbf{Sliding window} & \textbf{Layers} & \textbf{MSE}\\
        \hline
        4 & (100, 50, 25, 20, 10) & 18889414\\
        4 & (10, 10, 10, 10, 10, 10) & 18935098\\
        4 & (100, 50, 50, 20, 10) & 18847516\\
        8 & (100, 50, 25, 20, 10) & 17044955\\
        8 & (150, 50, 50, 50, 50, 10) & 17216134\\
        8 & (50, 50, 50, 50) & 18394493\\
        16 & (10, 20, 30, 40, 50) & 18116299\\
        16 & (100, 20, 20, 20, 10) & 18466524\\
        16 & (10, 10, 10, 10, 10, 10, 10) & 18311036\\
      \end{tabular}
    \end{center}
  \end{table}

  \subsubsection{CNN}
  Firstly, a baseline model was selected through manual experimentation. This had the following structure:
  intput of size 20, a 1d convolutional layer, a maxpooling layer, a flatten layer, a dense layer of size 150 
  and the output layer. The same batch size, epoch no grid search was performed and it yielded similar results 
  to \ref{fig:epoch_batch}. This was followed by iterating the same optimizers and activation function in 
  order to select Adadelta and softplus. 

  \begin{table}
    \begin{center}
      \caption{CNN layers and size tuning}
      \label{tab:layers_cnn}
      \begin{tabular}{c|c|c}
        \textbf{Sliding window} & \textbf{Layers} & \textbf{MSE}\\
        \hline
        8 & (25, 10, 5) & 35385451\\
        64 & (100, 20, 10, 5) & 35012864\\
        128 & (100, 20, 10, 5) & 21086942\\
        128 & (300, 50) & 22287266\\
        128 & (10, 10, 10) & 23441869\\
        256 & (100,20,10,5) & 23783826\\
      \end{tabular}
    \end{center}
  \end{table}

  The main idea behind using a CNN model for this task is to pass a larger history window as input. 
  CNN layers build different filter that learn certain characteristics of the input data. They are well 
  suited for data which has a spatial relationship(like timeseries)\cite{cnn}.

  \subsubsection{CNN-LSTM Hybrid}
  This model was applied on a range of timeseries tasks by Lin et al.\cite{cnn_lstm}.
  It relies on LSTM to find long range dependencies and historical trends and on CNN 
  to extract important features from raw timeseries data. The starting values for 
  some parameters were influenced by this research: 32 cnn filters, 1 lstm layer with 
  a couple hundred units.

  \begin{table}
    \begin{center}
      \caption{CNN-LSTM layers and size tuning}
      \label{tab:layers_cnn_lstm}
      \begin{tabular}{c|c|c}
        \textbf{Sliding window} & \textbf{LSTM layers} & \textbf{MSE}\\
        \hline
        144 & (20, 10, 5) & 51700082\\
        todo & todo & todo\\
      \end{tabular}
    \end{center}
  \end{table}


  \subsection{Experiment Results}

  Each of the most promising models were then trained again on all training data,
  for a larger number of epochs and repeated multiple times to account for the 
  random weight initialization. The best results were then compared to baselines
  and to eachother as seen in table \ref{tab:final_eval}.

  \begin{table}[h]
    \begin{center}
      \caption{Final results}
      \label{tab:final_eval}
      \begin{tabular}{c|c|c|c|c|c}
        \textbf{Dataset} & \textbf{Naive} & \textbf{ARIMA} & \textbf{MLP} & \textbf{CNN} & \textbf{CNN-LSTM} \\
        \hline
        Jp10 & 20233320 & 15289199 & 8591086 & 12766376 & 7252806 \\
        Jp15 & 87883950 & 56662039 & 31042348 & 36278866 & 57719162 \\
        De10 & 16789198 & todo & todo & todo & todo \\
        De15 & 77481580 & todo & todo & todo & todo \\
      \end{tabular}
    \end{center}
  \end{table}

  TODO t=5,10,15
  TODO german wiki 5,10,15
  TODO compare with cloudinsight article

  \newpage
  \bibliography{bibliography}{}
  \bibliographystyle{plain}
\end{document}