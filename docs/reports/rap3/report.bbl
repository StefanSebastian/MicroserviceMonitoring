\begin{thebibliography}{10}

\bibitem{sliding_window}
Gianluca Bontempi, Souhaib Ben~Taieb, and Yann-A{\"e}l Le~Borgne.
\newblock {\em Machine Learning Strategies for Time Series Forecasting}, pages
  62--77.
\newblock Springer Berlin Heidelberg, Berlin, Heidelberg, 2013.

\bibitem{Dickey-Fuller}
Yin-Wong Cheung and Kon~S Lai.
\newblock Lag order and critical values of the augmented dickey--fuller test.
\newblock {\em Journal of Business \& Economic Statistics}, 13(3):277--280,
  1995.

\bibitem{scryer}
Neeraj~Joshi Daniel~Jacobson, Danny~Yuan.
\newblock Scryer: Netflix’s predictive auto scaling engine, November 2013.
\newblock [Online; posted 05-November-2013].

\bibitem{arima}
S.~L. Ho and M.~Xie.
\newblock The use of arima models for reliability forecasting and analysis.
\newblock {\em Comput. Ind. Eng.}, 35(1–2):213–216, October 1998.

\bibitem{msc}
Anshul Jindal, Vladimir Podolskiy, and Michael Gerndt.
\newblock Performance modeling for cloud microservice applications.
\newblock pages 25--32, 04 2019.

\bibitem{batch_size}
Nitish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
  Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock 09 2016.

\bibitem{CloudInsight}
I.~K. {Kim}, W.~{Wang}, Y.~{Qi}, and M.~{Humphrey}.
\newblock Cloudinsight: Utilizing a council of experts to predict future cloud
  application workloads.
\newblock In {\em 2018 IEEE 11th International Conference on Cloud Computing
  (CLOUD)}, pages 41--48, 2018.

\bibitem{cnn}
Yann LeCun, Y.~Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em Nature}, 521:436--44, 05 2015.

\bibitem{cnn_lstm}
Tao Lin, Tian Guo, and Karl Aberer.
\newblock Hybrid neural networks for learning the trend in time series.
\newblock pages 2273--2279, 2017.

\bibitem{kfold}
Stuart Russell and Peter Norvig.
\newblock {\em Artificial Intelligence: A Modern Approach}.
\newblock Prentice Hall Press, USA, 3rd edition, 2009.

\bibitem{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(56):1929--1958, 2014.

\end{thebibliography}
